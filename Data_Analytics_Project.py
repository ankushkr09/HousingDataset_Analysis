# -*- coding: utf-8 -*-
"""Assignment_Solution_Ankush_Jha.ipynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1afz9XNyBuwGmHXHQyE8mix2pIKd42nFK
"""

#Importing neccessary libraries
from google.colab import drive
import pandas as pd
from sqlalchemy import create_engine, text

#Connecting CSV file with notebook from google drive
drive.mount('/content/drive', force_remount = True)
filepath = "/content/drive/My Drive/HousingDataset.csv"
df = pd.read_csv(filepath, engine = 'python')


# SECTION-1: ENVIRONMENT SETUP AND DATA CLEANING


#Columns to investigate and remove from the table
remove_columns = ['ad_type', 'title', 'description', 'l4', 'l5', 'l6']
df_cleaned = df.drop(columns= remove_columns)

# Rows to discard if any of the column value is null
rows_to_check = ['lon', 'lat', 'price_period', 'bedrooms', 'surface_total', 'rooms', 'price', 'surface_covered']
df_cleaned = df_cleaned.dropna(subset=rows_to_check)

#Create dataframe of property details and property price
Property_Details_df = df_cleaned [['id', 'start_date', 'end_date', 'created_on', 'lat', 'lon', 'l1', 'l2', 'l3', 'rooms', 'bedrooms', 'bathrooms', 'surface_total', 'surface_covered']]
Property_Price_Details_df = df_cleaned [['id', 'price', 'currency', 'price_period', 'property_type', 'operation_type']]

#connect to sql  db
engine = create_engine('sqlite:///property_data.db')
connection = engine.connect()   #create database connection

#convert dataframe to sql
Property_Details_df.to_sql('Property_Details', engine, if_exists='replace')
Property_Price_Details_df.to_sql('Property_Price_Details', engine, if_exists='replace')


# frame select  query property  details
query0 = text("SELECT * FROM Property_Details")
#run select query for property details
result0 = connection.execute(query0)
#fetch results for property details
query_result0 = pd.DataFrame(result0.fetchall(), columns=result0.keys())


#SECTION-2 DATA ANALYSIS


# Query-1 (properties that have a price greater than 1 million and are located in "Estados Unidos" (l1))
query1 = text("""
SELECT * FROM Property_Price_Details
INNER JOIN Property_Details
ON Property_Price_Details.id = Property_Details.id
WHERE price > 1000000 AND l1 = 'Estados Unidos'
""")
result1 = connection.execute(query1)
query_result1 = pd.DataFrame(result1.fetchall(), columns=result1.keys())



#Query-2 (Categorize properties based on their surface area as 'Small' if it's less than 50 square meters, 'Medium' if it's between 50 and 100 square meters, and 'Large' if it's greater than 100 square meters:)
query2 = text("""
SELECT id, surface_total,
    CASE
        WHEN surface_total < 50 THEN 'Small'
        WHEN surface_total >= 50 AND surface_total <= 100 THEN 'Medium'
        ELSE 'Large'
    END AS surface_area_category
FROM Property_Details;
""")
result2 = connection.execute(query2)
query_result2 = pd.DataFrame(result2.fetchall(), columns=result2.keys())



#Query-3 (List all properties (id) in the "Belgrano" neighborhood (l3) haveing same number of bedrooms and bathrooms as another property in the dataset:)
query3 = text("""
SELECT p1.id FROM Property_Details AS p1
WHERE p1.l3 = 'Belgrano'
AND EXISTS (
    SELECT 1 FROM Property_Details AS p2
    WHERE p2.l3 = 'Belgrano'
    AND p1.id != p2.id
    AND p1.bedrooms = p2.bedrooms
    AND p1.bathrooms = p2.bathrooms
    );
""")
result3 = connection.execute(query3)
query_result3 = pd.DataFrame(result3.fetchall(), columns = result3.keys())



#Query-4 (Average price per square meter (price / surface_total) for each property type (property_type) in the "Belgrano" neighborhood (l3):)
query4 = text("""
SELECT ppd.Property_Type, AVG(ppd.price / pd.surface_total) AS avg_price_per_sqm
FROM Property_Price_Details AS ppd
INNER JOIN Property_Details AS pd
ON ppd.id = pd.id
WHERE pd.l3 = 'Belgrano'
GROUP BY ppd.property_type;
""")
result4 = connection.execute(query4)
query_result4 = pd.DataFrame(result4.fetchall(), columns = result4.keys())



#Query-5 (properties having higher price than the average price of properties with the same number of bedrooms and bathrooms.)
query5 = text("""
SELECT pd.id
FROM Property_Details AS pd
INNER JOIN Property_Price_Details AS ppd
ON pd.id = ppd.id
WHERE ppd.price > (
    SELECT AVG(ppd2.price)
    FROM Property_Price_Details AS ppd2
    INNER JOIN Property_Details AS pd2
    ON ppd2.id = pd2.id
    WHERE pd2.bedrooms = pd.bedrooms
    AND pd2.bathrooms = pd.bathrooms
)
LIMIT 100
""")
result5 = connection.execute(query5)
query_result5 = pd.DataFrame(result5.fetchall(), columns = result5.keys())


#Query-6 (Calculate the cumulative price for each property type, ordered by the creation date.)
query6 = text("""
SELECT ppd.property_type, pd.start_date, SUM(ppd.price) AS cumulative_price
FROM Property_Details AS pd
INNER JOIN Property_Price_Details AS ppd
ON pd.id = ppd.id
GROUP BY pd.id, ppd.property_type, pd.start_date
ORDER BY pd.start_date;
""")
result6 = connection.execute(query6)
query_result6 = pd.DataFrame(result6.fetchall(), columns = result6.keys())


#Query-7 (Identify the 10 locations (l3) with the highest total surface area (sum of surface_total) of properties listed for sale (operation_type = 'Venta'):)
connection = engine.connect()
query7 = text("""
SELECT l3, SUM(surface_total) AS total_surface_area
FROM Property_Details
INNER JOIN Property_Price_Details
ON Property_Details.id = Property_Price_Details.id
WHERE operation_type = 'Venta'
GROUP BY l3
ORDER BY total_surface_area DESC
LIMIT 10;
""")
result7 = connection.execute(query7)
query_result7 = pd.DataFrame(result7.fetchall(), columns = result7.keys())


#Query-8 (Find the top 5 most expensive properties (based on price) in the "Palermo" neighborhood (l3) that were listed in August 2020:)
query8 = text("""
SELECT pd.id, pd.created_on, pd.rooms, pd.bedrooms, pd.bathrooms, ppd.price, ppd.property_type, ppd.operation_type
FROM Property_Details AS pd
INNER JOIN Property_Price_Details AS ppd
ON pd.id = ppd.id
WHERE pd.l3 = 'Palermo' AND pd.start_date BETWEEN '8-1-2020' AND '9-1-2020'
ORDER BY ppd.price DESC
LIMIT 5;
""")
result8 = connection.execute(query8)
query_result8 = pd.DataFrame(result8.fetchall(), columns = result8.keys())


#Query-9 (Find the top 3 properties with the highest price per square meter (price divided by surface area) within each property type.)
query9 = text("""
WITH RankedProperties AS (
    SELECT pd.id, ppd.property_type, pd.surface_total, ppd.price, ppd.price / pd.surface_total AS price_per_sq_meter,
    ROW_NUMBER() OVER (PARTITION BY ppd.property_type ORDER BY ppd.price / pd.surface_total DESC) AS rn
    FROM Property_Details AS pd
    INNER JOIN Property_Price_Details AS ppd
    ON pd.id = ppd.id
)
SELECT id, property_type, price, price_per_sq_meter FROM RankedProperties
WHERE rn <= 3;
""")
result9 = connection.execute(query9)
query_result9 = pd.DataFrame(result9.fetchall(), columns = result9.keys())


#Query-10 (Find the top 3 locations (l1, l2, l3) with the highest average price per square meter (price / surface_total) for properties listed for sale (operation_type = 'Venta') in the year 2020. Exclude locations with fewer than 10 properties listed for sale in 2020 from the results.)
query10 = text("""
WITH FilteredProperties AS (
    SELECT pd.l1, pd.l2, pd.l3, ppd.price / pd.surface_total AS price_per_sq_meter
    FROM Property_Details AS pd
    INNER JOIN Property_Price_Details AS ppd
    ON pd.id = ppd.id
    WHERE ppd.operation_type = 'Venta' AND pd.start_date LIKE '%2020%'
),
LocationStats AS (
    SELECT l1, l2, l3, AVG(price_per_sq_meter) AS avg_price_per_sq_meter, COUNT(*) AS property_count
    FROM FilteredProperties
    GROUP BY l1, l2, l3
    HAVING COUNT(*) >= 10
)
SELECT l1, l2, l3, avg_price_per_sq_meter
FROM LocationStats
ORDER BY avg_price_per_sq_meter DESC
LIMIT 3;
""")
result10 = connection.execute(query10)
query_result10 = pd.DataFrame(result10.fetchall(), columns = result10.keys())

# print(query_result10)

#close connection to db
connection.close()


#SECTION-3 EXPOSE THE RESULTS IN API

#install ngrok and import neccessary libraries
!pip install pyngrok
from pyngrok import ngrok
from flask import Flask, jsonify

#creating a public url to use for API call
ngrok.set_auth_token("2Vy2LQq0ITbjrdD2RlTHDxCJvCG_4beDr1TCvhMMbkT9o1Tuk")
public_url = ngrok.connect(5000).public_url
print(f"Public URL: {public_url}")

app = Flask(__name__)

# creating a dictionary query results from section 2 and assigning the values to each queries respective of question number
query_results = {
    "question-1": query_result1.to_dict(orient='records'),
    "question-2": query_result2.to_dict(orient='records'),
    "question-3": query_result3.to_dict(orient='records'),
    "question-4": query_result4.to_dict(orient='records'),
    "question-5": query_result5.to_dict(orient='records'),
    "question-6": query_result6.to_dict(orient='records'),
    "question-7": query_result7.to_dict(orient='records'),
    "question-8": query_result8.to_dict(orient='records'),
    "question-9": query_result9.to_dict(orient='records'),
    "question-10": query_result10.to_dict(orient='records')
}
# API endpoint to retrieve answers to questions
@app.route('/<question>', methods=['GET'])
def get_answer(question):
    try:
        if question in query_results:
            return jsonify({f"Answer for {question} is ":query_results[question]})
        else:
            return jsonify({"Error": "Invalid Question Number"}), 404
    except Exception as e:
        return jsonify({"Error": str(e)}), 500

if __name__ == '__main__':
    app.run(port=5000)